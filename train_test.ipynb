{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_test.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMp4ZtV1MA/fECrzCK80UiE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmswl0707/ResNet_Pytorch/blob/main/train_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLR2DfuDZQwN"
      },
      "source": [
        "def train(epoch, model, data_loader):\r\n",
        "    print(\"==========training==========\")\r\n",
        "    running_loss = 0.0\r\n",
        "    running_acc = 0.0\r\n",
        "\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    for e in range(epoch):      #에폭 만큼 반복\r\n",
        "        for batch_idx, (input, target) in enumerate(data_loader):       # 배치 사이즈 만큼 반복\r\n",
        "            global_step = e * len(data_loader)*batch_size + (batch_idx * batch_size)\r\n",
        "\r\n",
        "            input, target = input.cuda(), target.cuda()\r\n",
        "            input, target = Variable(input, volatile=False), Variable(target)\r\n",
        "\r\n",
        "            optimizer.zero_grad()\r\n",
        "\r\n",
        "            output = model(input)\r\n",
        "\r\n",
        "            loss=F.nll_loss(output, target)\r\n",
        "            preds = output.data.max(dim=1, keepdim=True)[1]\r\n",
        "            acc=preds.eq(target.data.view_as(preds)).cpu().sum()\r\n",
        "            running_loss += loss.item()\r\n",
        "            running_acc+=acc.item()\r\n",
        "\r\n",
        "\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "\r\n",
        "            if batch_idx%100==0:\r\n",
        "                loss = running_loss/100\r\n",
        "                accuracy = running_acc/100\r\n",
        "                writer.add_scalar('loss', loss, global_step)        #tensorboard scalar add\r\n",
        "                writer.add_scalar('accuracy', accuracy, global_step)\r\n",
        "\r\n",
        "                print('Epoch: {:<4} iter: {:>4} loss {} acc {}' .format(e+1, global_step, loss, accuracy))\r\n",
        "                running_loss = 0.0\r\n",
        "                running_acc = 0.0\r\n",
        "\r\n",
        "def test(model, data_loader):\r\n",
        "    model.eval()\r\n",
        "    volatile = True\r\n",
        "    running_acc = 0.0\r\n",
        "\r\n",
        "    for batch_idx, (input, target) in enumerate(data_loader):\r\n",
        "        global_step = batch_idx * batch_size\r\n",
        "\r\n",
        "        input, target = input.cuda(), target.cuda()\r\n",
        "        input, target = Variable(input, volatile=True), Variable(target)\r\n",
        "\r\n",
        "        output = model(input)\r\n",
        "        preds = output.data.max(dim=1, keepdim=True)[1]\r\n",
        "        acc = preds.eq(target.data.view_as(preds)).cpu().sum()\r\n",
        "        running_acc += acc.item()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    accuracy = running_acc / len(data_loader)\r\n",
        "\r\n",
        "    print('test acc {}'.format(accuracy))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}